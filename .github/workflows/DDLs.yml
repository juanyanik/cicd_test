name: Export Snowflake DDLs

on:
  workflow_dispatch: # Trigger manually
  schedule:
    - cron: '0 0 * * *' # Runs daily at midnight (adjust as needed)

jobs:
  export-ddls:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python environment
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install Snowflake connector
        run: |
          python -m pip install --upgrade pip
          pip install snowflake-connector-python

      - name: Export Snowflake DDLs
        env:
          SF_ACCOUNT: ${{ secrets.SF_ACCOUNT }}
          SF_USER: ${{ secrets.SF_USER }}
          SF_PASSWORD: ${{ secrets.SF_PASSWORD }}
          SF_ROLE: ${{ secrets.SF_ROLE }}
          SF_WAREHOUSE: ${{ secrets.SF_WAREHOUSE }}
        run: |
          import snowflake.connector
          import os

          # Snowflake connection
          conn = snowflake.connector.connect(
              user=os.getenv('SF_USER'),
              password=os.getenv('SF_PASSWORD'),
              account=os.getenv('SF_ACCOUNT'),
              warehouse=os.getenv('SF_WAREHOUSE'),
              role=os.getenv('SF_ROLE')
          )

          cursor = conn.cursor()

          # Query to get all databases
          cursor.execute("SHOW DATABASES")
          databases = cursor.fetchall()

          # Create directories in the repo based on Snowflake structure
          repo_dir = './snowflake_ddls'
          os.makedirs(repo_dir, exist_ok=True)

          # Loop through each database and its objects
          for db in databases:
              db_name = db[1]
              cursor.execute(f"USE DATABASE {db_name}")

              # Get all schemas in the database
              cursor.execute("SHOW SCHEMAS")
              schemas = cursor.fetchall()

              for schema in schemas:
                  schema_name = schema[1]
                  cursor.execute(f"USE SCHEMA {schema_name}")

                  # Get all tables in the schema
                  cursor.execute("SHOW TABLES")
                  tables = cursor.fetchall()

                  for table in tables:
                      table_name = table[1]
                      table_ddl_query = f"SHOW CREATE TABLE {schema_name}.{table_name}"
                      cursor.execute(table_ddl_query)
                      ddl_result = cursor.fetchone()
                      ddl = ddl_result[0] if ddl_result else None

                      # Create subdirectories based on the database and schema
                      table_dir = os.path.join(repo_dir, db_name, schema_name)
                      os.makedirs(table_dir, exist_ok=True)

                      # Write the DDL to a file
                      if ddl:
                          ddl_file_path = os.path.join(table_dir, f"{table_name}_ddl.sql")
                          with open(ddl_file_path, "w") as f:
                              f.write(ddl)

          cursor.close()
          conn.close()

      - name: Commit and push changes
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add snowflake_ddls/
          git commit -m "Export Snowflake DDLs"
          git push
